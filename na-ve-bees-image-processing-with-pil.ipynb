{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tusharaggarwal27/na-ve-bees-image-processing-with-pil?scriptVersionId=124032944\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:250%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:150%;\n          color:blue;\">\n           üêùüçØNa√Øve BeesüêùüçØLearn Image Processing with PIL (Python Imaging Library)üêùüçØ\n</p>\n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:10px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          text-align: center;\n          color:black;\">\nFollow me on:\n\nüîó github.com/tushar2704\n    \nüîómedium.com/@tushar_aggarwal\n    \n üîókaggle.com/tusharaggarwal27\n    \nüîólinkedin.com/in/tusharaggarwalinseec\n\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:10px;\n           background-color:skyblue;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: left;\n          color:black;\">\nüêùüçØCan a machine distinguish between a honey bee and a bumble bee? Being able to identify bee species from images, while challenging, would allow researchers to more quickly and effectively collect field data. In this Notebook, I will show you how to use the Python image library Pillow to load and manipulate image data also you'll learn common transformations of images and how to build them into a pipeline.\n\n Note:I completely brewed this notebbok from cratch, if you learn from it please upvote and cite me if sharing on other platforms.üêùüçØ\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:skyblue;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\n<p><img src=\"https://assets.datacamp.com/production/project_374/img/honey.jpg\" alt=\"honey bee\">\n<em>A honey bee.</em></p>\n<p>The question at hand is: can a machine identify a bee as a honey bee or a bumble bee? These bees have different <a href=\"http://bumblebeeconservation.org/about-bees/faqs/honeybees-vs-bumblebees/\">behaviors and appearances</a>, but given the variety of backgrounds, positions, and image resolutions it can be a challenge for machines to tell them apart.</p>\n<p>Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data. Pollinating bees have critical roles in both ecology and agriculture, and diseases like <a href=\"http://news.harvard.edu/gazette/story/2015/07/pesticide-found-in-70-percent-of-massachusetts-honey-samples/\">colony collapse disorder</a> threaten these species. Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects.</p>\n<p><img src=\"https://assets.datacamp.com/production/project_374/img/bumble.jpg\" alt=\"bumble bee\">\n<em>A bumble bee.</em></p>\n</p>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"3"},"run_control":{"frozen":true}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nImporting the required libraries\n\n</p>","metadata":{}},{"cell_type":"code","source":"from pathlib import Path # Used to change filepaths\n# We set up matplotlib, pandas, and the display function\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport pandas as pd\nimport numpy as np\n\nfrom PIL import Image # import Image from PIL so we can use it later","metadata":{"tags":["sample_code"],"dc":{"key":"3"},"execution":{"iopub.status.busy":"2022-12-27T16:46:12.612203Z","iopub.execute_input":"2022-12-27T16:46:12.612625Z","iopub.status.idle":"2022-12-27T16:46:12.619868Z","shell.execute_reply.started":"2022-12-27T16:46:12.61259Z","shell.execute_reply":"2022-12-27T16:46:12.618828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nGenerating the test_data\n\n</p>","metadata":{}},{"cell_type":"code","source":"test_data= np.random.beta(1,1, size=(100, 100, 3))\n\n# displaying the test_data\nplt.imshow(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-27T16:46:12.625092Z","iopub.execute_input":"2022-12-27T16:46:12.625594Z","iopub.status.idle":"2022-12-27T16:46:12.955324Z","shell.execute_reply.started":"2022-12-27T16:46:12.625534Z","shell.execute_reply":"2022-12-27T16:46:12.954312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n           Now that we have all of our imports ready, it is time to work with some real images.\n\nPillow is a very flexible image loading and manipulation library. It works with many different image formats, for example, .png, .jpg, .gif and more. For most image data, one can work with images using the Pillow library (which is imported as PIL).\n\nNow we want to load an image, display it in the notebook, and print out the dimensions of the image. By dimensions, we mean the width of the image and the height of the image. These are measured in pixels. The documentation for Image in Pillow gives a comprehensive view of what this object can do.\nPlease find images at https://www.kaggle.com/datasets/tusharaggarwal27/beesimage\n</p>\n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nOpening images with PIL\n\n</p>","metadata":{}},{"cell_type":"code","source":"\nimg = Image.open('/kaggle/input/beesimage/bees1.png') # opening the image\n\nimg_size = img.size # Getting the image size\n\nprint(\"The image size is: {}\".format(img_size))\n\nimg","metadata":{"tags":["sample_code"],"dc":{"key":"10"},"execution":{"iopub.status.busy":"2022-12-27T16:46:12.957534Z","iopub.execute_input":"2022-12-27T16:46:12.958024Z","iopub.status.idle":"2022-12-27T16:46:12.972671Z","shell.execute_reply.started":"2022-12-27T16:46:12.957986Z","shell.execute_reply":"2022-12-27T16:46:12.971362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\n Image manipulation with PIL\n\n</p>","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"10"},"run_control":{"frozen":true}}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n           Pillow has a number of common image manipulation tasks built into the library. For example, one may want to resize an image so that the file size is smaller. Or, perhaps, convert an image to black-and-white instead of color. Operations that Pillow provides include:resizing,cropping,rotating,flipping.\n    converting to greyscale (or other <a href=\"https://pillow.readthedocs.io/en/5.1.x/handbook/concepts.html#concept-modes\">color modes</a>)\n    Often, these kinds of manipulations are part of the pipeline for turning a small number of images into more images to create training data for machine learning algorithms. This technique is called <a href=\"http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf\">data augmentation</a>, and it is a common technique for image classification.\n</p>\n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"17"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"# Cropping the image to 25, 25, 75, 75\nimg_cropped = img.crop([25, 25, 75, 75])\ndisplay(img_cropped)\n\n# Rotating the image by 45 degrees\nimg_rotated = img.rotate(45, expand=[25])\ndisplay(img_rotated)\n\n# flip the image left to right\nimg_flipped = img.transpose(Image.FLIP_LEFT_RIGHT)\ndisplay(img_flipped)","metadata":{"tags":["sample_code"],"dc":{"key":"17"},"execution":{"iopub.status.busy":"2022-12-27T16:46:12.974043Z","iopub.execute_input":"2022-12-27T16:46:12.974419Z","iopub.status.idle":"2022-12-27T16:46:12.995317Z","shell.execute_reply.started":"2022-12-27T16:46:12.974388Z","shell.execute_reply":"2022-12-27T16:46:12.994399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\n Images as arrays of data\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n      What is an image? So far, PIL has handled loading images and displaying them. However, if we're going to use images as data, we need to understand what that data looks like.\nMost image formats have three color \"channels\": red, green, and blue (some images also have a fourth channel called \"alpha\" that controls transparency). For each pixel in an image, there is a value for every channel. The way this is represented as data is as a three-dimensional matrix. The width of the matrix is the width of the image, the height of the matrix is the height of the image, and the depth of the matrix is the number of channels. So, as we saw, the height and width of our image are both 100 pixels. This means that the underlying data is a matrix with the dimensions 100x100x3.  \n    \n    \n    \n\n    \n</p>\n    <img src=\"https://assets.datacamp.com/production/project_374/img/AdditiveColor.png\" alt=\"RGB Colors\">\n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{}},{"cell_type":"code","source":"# Turning our image object into a NumPy array\nimg_data = np.array(img)\n\n# getting the shape of the resulting array\nimg_data_shape = img_data.shape\n\nprint(\"Our NumPy array has the shape: {}\".format(img_data_shape))\n\n# plotting the data with `imshow` \nplt.imshow(img_data)\nplt.show()\n\n# plotting the red channel\nplt.imshow(img_data[:,:,0], cmap=plt.cm.Reds_r)\nplt.show()\n\n# plotting the green channel\nplt.imshow(img_data[:,:,1], cmap=plt.cm.Greens_r)\nplt.show()\n\n# plotting the blue channel\nplt.imshow(img_data[:,:,2], cmap=plt.cm.Blues_r)\nplt.show()","metadata":{"tags":["sample_code"],"dc":{"key":"24"},"execution":{"iopub.status.busy":"2022-12-27T16:46:12.998338Z","iopub.execute_input":"2022-12-27T16:46:12.998818Z","iopub.status.idle":"2022-12-27T16:46:13.563886Z","shell.execute_reply.started":"2022-12-27T16:46:12.998778Z","shell.execute_reply":"2022-12-27T16:46:13.562816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nExploring the color channels\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n      Color channels can help provide more information about an image. A picture of the ocean will be more blue, whereas a picture of a field will be more green. This kind of information can be useful when building models or examining the differences between images.\n    We'll look at the <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\">kernel density estimate</a> for each of the color channels on the same plot so that we can understand how they differ.\n    When we make this plot, we'll see that a shape that appears further to the right means more of that color, whereas further to the left means less of that color.\n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"31"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"def plot_kde(channel, color):\n    \"\"\" Plots a kernel density estimate for the given data.\n        \n        `channel` must be a 2d array\n        `color` must be a color string, e.g. 'r', 'g', or 'b'\n    \"\"\"\n    data = channel.flatten()\n    return pd.Series(data).plot.density(c=color)\n\n# creating the list of channels\nchannels = (['r', 'g', 'b'])\n    \ndef plot_rgb(image_data):\n    # using enumerate to loop over colors and indexes\n    for ix, color in enumerate(channels):\n        plot_kde(img_data[:,:,ix], color)\n\n    plt.show()\n    \nplot_rgb(img_data)","metadata":{"tags":["sample_code"],"dc":{"key":"31"},"execution":{"iopub.status.busy":"2022-12-27T16:46:13.565236Z","iopub.execute_input":"2022-12-27T16:46:13.56599Z","iopub.status.idle":"2022-12-27T16:46:14.117969Z","shell.execute_reply.started":"2022-12-27T16:46:13.565948Z","shell.execute_reply":"2022-12-27T16:46:14.116745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nHoney bees and bumble bees (i)\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n     Now we'll look at two different images and some of the differences between them. The first image is of a honey bee, and the second image is of a bumble bee.First, let's look at the honey bee.\n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"38"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"# loading image2 as honey\nhoney= Image.open('/kaggle/input/beesimage/bees2.png')\n\n# displaying the honey bee image\ndisplay(honey)\n\n# NumPy array of the honey bee image data\nhoney_data = np.array(honey)\n\n# plotting the rgb densities for the honey bee image\nplot_rgb(honey_data)","metadata":{"tags":["sample_code"],"dc":{"key":"38"},"execution":{"iopub.status.busy":"2022-12-27T16:46:14.11929Z","iopub.execute_input":"2022-12-27T16:46:14.119585Z","iopub.status.idle":"2022-12-27T16:46:14.677974Z","shell.execute_reply.started":"2022-12-27T16:46:14.119547Z","shell.execute_reply":"2022-12-27T16:46:14.676859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nHoney bees and bumble bees (ii)\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n     Now let's look at the bumble bee.</p>\nWhen one compares these images, it is clear how different the colors are. The honey bee image above, with a blue flower, has a strong peak on the right-hand side of the blue channel. The bumble bee image, which has a lot of yellow for the bee and the background, has almost perfect overlap between the red and green channels (which together make yellow).\n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"45"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"# loading image3 as bumble\nbumble = Image.open('/kaggle/input/beesimage/bees3.png')\n\n# displaying the bumble bee image\ndisplay(bumble)\n\n# NumPy array of the bumble bee image data\nbumble_data = np.array(bumble)\n\n# plotting the rgb densities for the bumble bee image\nplot_rgb(bumble_data)","metadata":{"tags":["sample_code"],"dc":{"key":"45"},"execution":{"iopub.status.busy":"2022-12-27T16:46:14.679736Z","iopub.execute_input":"2022-12-27T16:46:14.680187Z","iopub.status.idle":"2022-12-27T16:46:15.237932Z","shell.execute_reply.started":"2022-12-27T16:46:14.680145Z","shell.execute_reply":"2022-12-27T16:46:15.236887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nSimplify, simplify, simplify\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n     While sometimes color information is useful, other times it can be distracting. In this examples where we are looking at bees, the bees themselves are very similar colors. On the other hand, the bees are often on top of different color flowers. We know that the colors of the flowers may be distracting from separating honey bees from bumble bees, so let's convert these images to <a href=\"https://en.wikipedia.org/wiki/Grayscale\">black-and-white, or \"grayscale.\"</a>\n Grayscale is just one of the <a href=\"https://pillow.readthedocs.io/en/5.0.0/handbook/concepts.html#modes\">modes that Pillow supports</a>. Switching between modes is done with the <code>.convert()</code> method, which is passed a string for the new mode.\n  Because we change the number of color \"channels,\" the shape of our array changes with this change. It also will be interesting to look at how the KDE of the grayscale version compares to the RGB version above.  \n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"52"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"# converting honey to grayscale\nhoney_bw = img.convert(\"L\")\ndisplay(honey_bw)\n\n# converting the image to a NumPy array\nhoney_bw_arr = np.array(honey_bw)\n\n# getting the shape of the resulting array\nhoney_bw_arr_shape = honey_bw_arr.shape\nprint(\"Our NumPy array has the shape: {}\".format(honey_bw_arr_shape))\n\n# plotting the array using matplotlib\nplt.imshow(honey_bw_arr, cmap=plt.cm.gray)\nplt.show()\n\n# plotting the kde of the new black and white array\nplot_kde(honey_bw_arr, 'k')","metadata":{"tags":["sample_code"],"dc":{"key":"52"},"execution":{"iopub.status.busy":"2022-12-27T16:46:15.239368Z","iopub.execute_input":"2022-12-27T16:46:15.242856Z","iopub.status.idle":"2022-12-27T16:46:15.67418Z","shell.execute_reply.started":"2022-12-27T16:46:15.242822Z","shell.execute_reply":"2022-12-27T16:46:15.673051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nSaving the work!\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n    We've been talking this whole time about making changes to images and the manipulations that might be useful as part of a machine learning pipeline. To use these images in the future, we'll have to save our work after we've made changes.\nNow, we'll make a couple changes to the <code>Image</code> object from Pillow and save that. We'll flip the image left-to-right, just as we did with the color version. Then, we'll change the NumPy version of the data by clipping it. Using the <code>np.maximum</code> function, we can take any number in the array smaller than <code>100</code> and replace it with <code>100</code>. Because this reduces the range of values, it will increase the <a href=\"https://en.wikipedia.org/wiki/Contrast_(vision)\">contrast of the image</a>. We'll then convert that back to an <code>Image</code> and save the result.\n  \n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"59"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"# flipping the image left-right with transpose\nhoney_bw_flip =honey_bw.transpose(Image.FLIP_LEFT_RIGHT)\n\n# showing the flipped image\ndisplay(honey_bw_flip)\n\n# saving the flipped image\nhoney_bw_flip.save(\"bw_flipped.jpg\")\n\n# creating higher contrast by reducing range\nhoney_hc_arr=np.maximum(honey_bw_arr, 100)\n\n# showing the higher contrast version\nplt.imshow(honey_hc_arr, cmap=plt.cm.gray)\n\n# converting the NumPy array of high contrast to an Image\nhoney_bw_hc =Image.fromarray(honey_hc_arr)\n\n# saving the high contrast version\nhoney_bw_hc.save(\"bw_hc.jpg\")","metadata":{"tags":["sample_code"],"dc":{"key":"59"},"execution":{"iopub.status.busy":"2022-12-27T16:46:15.675915Z","iopub.execute_input":"2022-12-27T16:46:15.676611Z","iopub.status.idle":"2022-12-27T16:46:15.824511Z","shell.execute_reply.started":"2022-12-27T16:46:15.676548Z","shell.execute_reply":"2022-12-27T16:46:15.823277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n          text-align: center;\n          color:black;\">\nMaking a pipeline\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:2px;\n           background-color:#F5DEB3;\n           font-size:111%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:111%;\n          color:blue;\">\n \n  Now it's time to create an image processing pipeline. We have all the tools in our toolbox to load images, transform them, and save the results.\n    <p>In this pipeline we will do the following:</p>\n<ul>\n<li>Load the image with <code>Image.open</code> and create paths to save our images to</li>\n<li>Convert the image to grayscale</li>\n<li>Save the grayscale image</li>\n<li>Rotate, crop, and zoom in on the image and save the new image</li>\n</ul>\n    \n</p>\n  \n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>\n\n\n\n\n\n\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"66"},"run_control":{"frozen":true}}},{"cell_type":"code","source":"image_paths = ['/kaggle/input/beesimage/bees1.png', '/kaggle/input/beesimage/bees2.png', '/kaggle/input/beesimage/bees3.png']\n\ndef process_image(path):\n    img = Image.open(path)\n\n    # creating paths to save files to\n    bw_path = \"bw_{}.jpg\".format(path.stem)\n    rcz_path = \"rcz_{}.jpg\".format(path.stem)\n\n    print(\"Creating grayscale version of {} and saving to {}.\".format(path, bw_path))\n    bw = img.convert(\"L\")\n    bw.save(bw_path)\n    \n    print(\"Creating rotated, cropped, and zoomed version of {} and saving to {}.\".format(path, rcz_path))\n    rcz = bw.rotate(45).crop([25, 25, 75, 75]).resize((100, 100))\n    rcz.save(rcz_path)\n\n# for loop over image paths\nfor img_path in image_paths:\n    process_image(Path(img_path))","metadata":{"tags":["sample_code"],"dc":{"key":"66"},"execution":{"iopub.status.busy":"2022-12-27T16:46:15.828046Z","iopub.execute_input":"2022-12-27T16:46:15.828439Z","iopub.status.idle":"2022-12-27T16:46:15.844484Z","shell.execute_reply.started":"2022-12-27T16:46:15.828407Z","shell.execute_reply":"2022-12-27T16:46:15.843161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:10px;\n           background-color:orange;\n           font-size:120%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          text-align: center;\n          color:black;\">\nFollow me on:\n\nüîó github.com/tushar2704\n    \nüîómedium.com/@tushar_aggarwal\n    \n üîókaggle.com/tusharaggarwal27\n    \nüîólinkedin.com/in/tusharaggarwalinseec\n\n</p>\n","metadata":{"tags":["context"],"deletable":false,"dc":{"key":"24"},"run_control":{"frozen":true},"execution":{"iopub.status.busy":"2022-12-27T16:21:26.53398Z","iopub.execute_input":"2022-12-27T16:21:26.534423Z","iopub.status.idle":"2022-12-27T16:21:26.540472Z","shell.execute_reply.started":"2022-12-27T16:21:26.534389Z","shell.execute_reply":"2022-12-27T16:21:26.538779Z"}}}]}